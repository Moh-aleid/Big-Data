{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "import time\n",
        "import os\n",
        "import gzip  # <-- 1. IMPORT THE GZIP MODULE\n",
        "\n",
        "# --- Step 1: Generate a Large CSV File ---\n",
        "print(\"Generating a large CSV file...\")\n",
        "\n",
        "# Define the number of rows and chunks for generation\n",
        "total_rows = 100_000_000\n",
        "chunk_size = 500_000\n",
        "num_chunks = total_rows // chunk_size\n",
        "\n",
        "# Define column data\n",
        "categories = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "# Open a file to write to\n",
        "with open('large_dataset.csv', 'w') as f:\n",
        "    # Write the header\n",
        "    f.write('id,timestamp,category,value\\n')\n",
        "\n",
        "    # Generate and write data in chunks to avoid memory issues during creation\n",
        "    for i in range(num_chunks):\n",
        "        # Create a chunk of data\n",
        "        chunk_df = pd.DataFrame({\n",
        "            'id': range(i * chunk_size, (i + 1) * chunk_size),\n",
        "            'timestamp': pd.date_range(start='2023-01-01', periods=chunk_size, freq='s'),\n",
        "            'category': np.random.choice(categories, chunk_size),\n",
        "            'value': np.random.rand(chunk_size) * 100\n",
        "        })\n",
        "        # Append chunk to the CSV file (without the header)\n",
        "        chunk_df.to_csv(f, header=False, index=False)\n",
        "        print(f\"Generated chunk {i+1}/{num_chunks}\")\n",
        "\n",
        "print(\"\\n'large_dataset.csv' created successfully.\")\n",
        "\n",
        "\n",
        "# --- Step 1: Create the compressed file without loading the whole original file into memory ---\n",
        "print(\"\\nStep 1: Compressing the file (this may take a while)...\")\n",
        "# We read the original large CSV in chunks and write each chunk to a new compressed file.\n",
        "# This avoids the out-of-memory error during the compression process.\n",
        "chunk_iterator_for_compression = pd.read_csv('large_dataset.csv', chunksize=chunk_size)\n",
        "\n",
        "# We need to write the header first, then append the data chunks.\n",
        "# Use gzip.open to actually compress the data as it's written.\n",
        "first_chunk = True\n",
        "# --- 2. USE gzip.open() INSTEAD OF open() ---\n",
        "with gzip.open('large_dataset.csv.gz', 'wt') as f_out:\n",
        "    for chunk in chunk_iterator_for_compression:\n",
        "        chunk.to_csv(f_out, header=first_chunk, index=False)\n",
        "        first_chunk = False # Ensure header is only written once\n",
        "\n",
        "print(\"File compressed to 'large_dataset.csv.gz'\")\n",
        "original_size = os.path.getsize('large_dataset.csv') / (1024**3) # in GB\n",
        "compressed_size = os.path.getsize('large_dataset.csv.gz') / (1024**3) # in GB\n",
        "print(f\"Original file size: {original_size:.2f} GB\")\n",
        "print(f\"Compressed file size: {compressed_size:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neldWWfJJ6mN",
        "outputId": "ee2a5b56-6465-459f-b37d-19581bdeba31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating a large CSV file...\n",
            "Generated chunk 1/200\n",
            "Generated chunk 2/200\n",
            "Generated chunk 3/200\n",
            "Generated chunk 4/200\n",
            "Generated chunk 5/200\n",
            "Generated chunk 6/200\n",
            "Generated chunk 7/200\n",
            "Generated chunk 8/200\n",
            "Generated chunk 9/200\n",
            "Generated chunk 10/200\n",
            "Generated chunk 11/200\n",
            "Generated chunk 12/200\n",
            "Generated chunk 13/200\n",
            "Generated chunk 14/200\n",
            "Generated chunk 15/200\n",
            "Generated chunk 16/200\n",
            "Generated chunk 17/200\n",
            "Generated chunk 18/200\n",
            "Generated chunk 19/200\n",
            "Generated chunk 20/200\n",
            "Generated chunk 21/200\n",
            "Generated chunk 22/200\n",
            "Generated chunk 23/200\n",
            "Generated chunk 24/200\n",
            "Generated chunk 25/200\n",
            "Generated chunk 26/200\n",
            "Generated chunk 27/200\n",
            "Generated chunk 28/200\n",
            "Generated chunk 29/200\n",
            "Generated chunk 30/200\n",
            "Generated chunk 31/200\n",
            "Generated chunk 32/200\n",
            "Generated chunk 33/200\n",
            "Generated chunk 34/200\n",
            "Generated chunk 35/200\n",
            "Generated chunk 36/200\n",
            "Generated chunk 37/200\n",
            "Generated chunk 38/200\n",
            "Generated chunk 39/200\n",
            "Generated chunk 40/200\n",
            "Generated chunk 41/200\n",
            "Generated chunk 42/200\n",
            "Generated chunk 43/200\n",
            "Generated chunk 44/200\n",
            "Generated chunk 45/200\n",
            "Generated chunk 46/200\n",
            "Generated chunk 47/200\n",
            "Generated chunk 48/200\n",
            "Generated chunk 49/200\n",
            "Generated chunk 50/200\n",
            "Generated chunk 51/200\n",
            "Generated chunk 52/200\n",
            "Generated chunk 53/200\n",
            "Generated chunk 54/200\n",
            "Generated chunk 55/200\n",
            "Generated chunk 56/200\n",
            "Generated chunk 57/200\n",
            "Generated chunk 58/200\n",
            "Generated chunk 59/200\n",
            "Generated chunk 60/200\n",
            "Generated chunk 61/200\n",
            "Generated chunk 62/200\n",
            "Generated chunk 63/200\n",
            "Generated chunk 64/200\n",
            "Generated chunk 65/200\n",
            "Generated chunk 66/200\n",
            "Generated chunk 67/200\n",
            "Generated chunk 68/200\n",
            "Generated chunk 69/200\n",
            "Generated chunk 70/200\n",
            "Generated chunk 71/200\n",
            "Generated chunk 72/200\n",
            "Generated chunk 73/200\n",
            "Generated chunk 74/200\n",
            "Generated chunk 75/200\n",
            "Generated chunk 76/200\n",
            "Generated chunk 77/200\n",
            "Generated chunk 78/200\n",
            "Generated chunk 79/200\n",
            "Generated chunk 80/200\n",
            "Generated chunk 81/200\n",
            "Generated chunk 82/200\n",
            "Generated chunk 83/200\n",
            "Generated chunk 84/200\n",
            "Generated chunk 85/200\n",
            "Generated chunk 86/200\n",
            "Generated chunk 87/200\n",
            "Generated chunk 88/200\n",
            "Generated chunk 89/200\n",
            "Generated chunk 90/200\n",
            "Generated chunk 91/200\n",
            "Generated chunk 92/200\n",
            "Generated chunk 93/200\n",
            "Generated chunk 94/200\n",
            "Generated chunk 95/200\n",
            "Generated chunk 96/200\n",
            "Generated chunk 97/200\n",
            "Generated chunk 98/200\n",
            "Generated chunk 99/200\n",
            "Generated chunk 100/200\n",
            "Generated chunk 101/200\n",
            "Generated chunk 102/200\n",
            "Generated chunk 103/200\n",
            "Generated chunk 104/200\n",
            "Generated chunk 105/200\n",
            "Generated chunk 106/200\n",
            "Generated chunk 107/200\n",
            "Generated chunk 108/200\n",
            "Generated chunk 109/200\n",
            "Generated chunk 110/200\n",
            "Generated chunk 111/200\n",
            "Generated chunk 112/200\n",
            "Generated chunk 113/200\n",
            "Generated chunk 114/200\n",
            "Generated chunk 115/200\n",
            "Generated chunk 116/200\n",
            "Generated chunk 117/200\n",
            "Generated chunk 118/200\n",
            "Generated chunk 119/200\n",
            "Generated chunk 120/200\n",
            "Generated chunk 121/200\n",
            "Generated chunk 122/200\n",
            "Generated chunk 123/200\n",
            "Generated chunk 124/200\n",
            "Generated chunk 125/200\n",
            "Generated chunk 126/200\n",
            "Generated chunk 127/200\n",
            "Generated chunk 128/200\n",
            "Generated chunk 129/200\n",
            "Generated chunk 130/200\n",
            "Generated chunk 131/200\n",
            "Generated chunk 132/200\n",
            "Generated chunk 133/200\n",
            "Generated chunk 134/200\n",
            "Generated chunk 135/200\n",
            "Generated chunk 136/200\n",
            "Generated chunk 137/200\n",
            "Generated chunk 138/200\n",
            "Generated chunk 139/200\n",
            "Generated chunk 140/200\n",
            "Generated chunk 141/200\n",
            "Generated chunk 142/200\n",
            "Generated chunk 143/200\n",
            "Generated chunk 144/200\n",
            "Generated chunk 145/200\n",
            "Generated chunk 146/200\n",
            "Generated chunk 147/200\n",
            "Generated chunk 148/200\n",
            "Generated chunk 149/200\n",
            "Generated chunk 150/200\n",
            "Generated chunk 151/200\n",
            "Generated chunk 152/200\n",
            "Generated chunk 153/200\n",
            "Generated chunk 154/200\n",
            "Generated chunk 155/200\n",
            "Generated chunk 156/200\n",
            "Generated chunk 157/200\n",
            "Generated chunk 158/200\n",
            "Generated chunk 159/200\n",
            "Generated chunk 160/200\n",
            "Generated chunk 161/200\n",
            "Generated chunk 162/200\n",
            "Generated chunk 163/200\n",
            "Generated chunk 164/200\n",
            "Generated chunk 165/200\n",
            "Generated chunk 166/200\n",
            "Generated chunk 167/200\n",
            "Generated chunk 168/200\n",
            "Generated chunk 169/200\n",
            "Generated chunk 170/200\n",
            "Generated chunk 171/200\n",
            "Generated chunk 172/200\n",
            "Generated chunk 173/200\n",
            "Generated chunk 174/200\n",
            "Generated chunk 175/200\n",
            "Generated chunk 176/200\n",
            "Generated chunk 177/200\n",
            "Generated chunk 178/200\n",
            "Generated chunk 179/200\n",
            "Generated chunk 180/200\n",
            "Generated chunk 181/200\n",
            "Generated chunk 182/200\n",
            "Generated chunk 183/200\n",
            "Generated chunk 184/200\n",
            "Generated chunk 185/200\n",
            "Generated chunk 186/200\n",
            "Generated chunk 187/200\n",
            "Generated chunk 188/200\n",
            "Generated chunk 189/200\n",
            "Generated chunk 190/200\n",
            "Generated chunk 191/200\n",
            "Generated chunk 192/200\n",
            "Generated chunk 193/200\n",
            "Generated chunk 194/200\n",
            "Generated chunk 195/200\n",
            "Generated chunk 196/200\n",
            "Generated chunk 197/200\n",
            "Generated chunk 198/200\n",
            "Generated chunk 199/200\n",
            "Generated chunk 200/200\n",
            "\n",
            "'large_dataset.csv' created successfully.\n",
            "\n",
            "Step 1: Compressing the file (this may take a while)...\n",
            "File compressed to 'large_dataset.csv.gz'\n",
            "Original file size: 4.57 GB\n",
            "Compressed file size: 1.46 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIVxjHoQF-U",
        "outputId": "05311c29-db86-4828-8f81-229814a5fb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Returns the current memory usage of the process in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss / (1024 ** 2) # Resident Set Size in MB\n",
        "# --- Method 1: Using Chunking ---\n",
        "print(\"\\n--- Method 1: Using pandas.read_csv(chunksize) ---\")\n",
        "start_time = time.time()\n",
        "initial_mem = get_memory_usage()\n",
        "peak_mem = initial_mem\n",
        "chunk_iterator = pd.read_csv('large_dataset.csv', chunksize=50000)\n",
        "total_sum = 0\n",
        "total_count = 0\n",
        "\n",
        "for chunk in chunk_iterator:\n",
        "    total_sum += chunk['value'].sum()\n",
        "    total_count += len(chunk)\n",
        "    current_mem = get_memory_usage()\n",
        "    if current_mem > peak_mem:\n",
        "        peak_mem = current_mem\n",
        "\n",
        "\n",
        "average_value = total_sum / total_count\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Calculated Average Value: {average_value:.4f}\")\n",
        "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Storage Impact: Uses the original file size on disk ({original_size:.2f}) GB.\")\n",
        "print(f\"Peak RAM used: {peak_mem - initial_mem:.2f} MB\")\n",
        "print(\"Memory Impact: Very low. Only one chunk is in memory at a time.\")"
      ],
      "metadata": {
        "id": "8FN_UK8tTeWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81178d84-97bd-40b0-9447-99147dc59330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Method 1: Using pandas.read_csv(chunksize) ---\n",
            "Execution Time: 66.49 seconds\n",
            "Calculated Average Value: 50.0031\n",
            "Storage Impact: Uses the original file size on disk (4.57) GB.\n",
            "Peak RAM used: 3.93 MB\n",
            "Memory Impact: Very low. Only one chunk is in memory at a time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Method 2: Using Dask ---\n",
        "print(\"\\n--- Method 2: Using Dask ---\")\n",
        "start_time = time.time()\n",
        "initial_mem = get_memory_usage()\n",
        "# Dask reads the CSV file and creates a Dask DataFrame (lazy evaluation)\n",
        "ddf = dd.read_csv('large_dataset.csv')\n",
        "\n",
        "# Perform the aggregation. This is still lazy.\n",
        "average_value_dask = ddf['value'].mean()\n",
        "\n",
        "# .compute() triggers the actual calculation\n",
        "average_value = average_value_dask.compute()\n",
        "peak_mem = get_memory_usage()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Calculated Average Value: {average_value:.4f}\")\n",
        "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Storage Impact: Uses the original file size on disk ({original_size:.2f}) GB.\")\n",
        "print(f\"Peak RAM used: {peak_mem - initial_mem:.2f} MB\")\n",
        "print(\"Memory Impact: Low. Dask intelligently manages memory and processes data in parallel.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZcoTG9KzboP",
        "outputId": "de3eedcd-9aef-414b-e202-8cdc6865b1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Method 2: Using Dask ---\n",
            "Calculated Average Value: 50.0031\n",
            "Execution Time: 73.22 seconds\n",
            "Storage Impact: Uses the original file size on disk (4.57) GB.\n",
            "Peak RAM used: 17.03 MB\n",
            "Memory Impact: Low. Dask intelligently manages memory and processes data in parallel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Method 3: Using Compression (Read in one go) ---\n",
        "print(\"\\n--- Method 3: Using Compression (Reading entire file at once) ---\")\n",
        "\n",
        "# --- Read the entire compressed file into memory at once ---\n",
        "\n",
        "print(\"This will only work if you have enough available RAM.\")\n",
        "\n",
        "start_time = time.time()\n",
        "initial_mem = get_memory_usage()\n",
        "try:\n",
        "    # This reads the whole .gz file into one DataFrame\n",
        "    df_compressed = pd.read_csv('large_dataset.csv.gz', compression='gzip')\n",
        "\n",
        "    # Now perform the calculation on the in-memory DataFrame\n",
        "    average_value = df_compressed['value'].mean()\n",
        "    peak_mem = get_memory_usage()\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    print(f\"Calculated Average Value: {average_value:.4f}\")\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(f\"Storage Impact: Uses the compressed file size on disk (~{compressed_size:.2f} MB).\")\n",
        "    print(f\"Peak RAM used: {peak_mem - initial_mem:.2f} MB\")\n",
        "    print(\"Memory Impact: HIGH. The entire decompressed dataset is loaded into RAM.\")\n",
        "\n",
        "except MemoryError:\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(\"\\nERROR: Out of memory!\")\n",
        "    print(f\"The script crashed after {execution_time:.2f} seconds.\")\n",
        "    print(\"The decompressed file is too large to fit into your system's RAM.\")\n",
        "    print(\"To process this file, you must use a memory-efficient method like Method 1 (chunking) or Method 2 (Dask).\")\n"
      ],
      "metadata": {
        "id": "9RHNeRLnHiLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKV88_DJ-W9Q"
      },
      "source": [
        "2. Comparison in Terms of Time, RAM and Storage\n",
        "Now, let's summarize the findings in a comparison table. The exact execution times will vary based on hardware (CPU speed, number of cores, disk type - SSD vs. HDD), but the relative performance trends are generally consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zQzGXRl-CRR"
      },
      "source": [
        "| Feature | `pandas.read_csv(chunksize)` | `Dask` | `Compression`|\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Execution Time** | **fastest** (depending on chunk size) | **fast** (Parallel processing) | **slowest** (Slower due to decompression overhead) |\n",
        "| **Storage (Disk)** | High (Original file size) | High (Original file size) | **Lowest** (Compressed file size, often 5-6x smaller) |\n",
        "| **Memory (RAM) Usage**| **Lowest** (Size of one chunk) | **Low** (Managed intelligently, slightly higher overhead than chunking) | **high** (Size of the decompressed file)(Original file size) |\n",
        "| **Ease of Use** | Easy (Standard pandas) but with aditional loop to read from each shunk | Easy (Familiar pandas-like API) | Easy (Two-step process, but straightforward) |\n",
        "| **Best For** | Simple, sequential tasks; when you can't install new libraries. | Complex analyses, aggregations, and operations that can be parallelized. | Saving disk space and speeding up I/O-bound tasks, especially on slower disks. Ideal for archiving datasets. \nbut its heavy to ram, so it's not recomended with low ram devices |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
